{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57d3157",
   "metadata": {},
   "source": [
    "# ETL Portion\n",
    "\n",
    "### Here, we are utilizing my personal data requests from both Facebook and Instagram, and looking through the messages in order to prepare them for sentiment analysis. The files are saved locally, which is why you will see local directories listed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51a0e0",
   "metadata": {},
   "source": [
    "# Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a10ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875533f",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7ce37",
   "metadata": {},
   "source": [
    "### First, we pull the data from the raw directory. Meta gives you a pull consisting of a folder for each conversation. These folders contain json files with the message contents. Longer Facebook Conversations can contain multiple jsons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e72b6",
   "metadata": {},
   "source": [
    "#### Instagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b43da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting folder names for each chat in order to iterate through later\n",
    "\n",
    "# insert your Instagram handle here:\n",
    "insta_account = 'REDACTED\n",
    "\n",
    "parent_directory = r'C:\\Users\\marco\\JUPYTER\\TextSentimentAnalysis\\instagram-{}\\messages\\inbox'.format(insta_account)\n",
    "\n",
    "items = os.listdir(parent_directory)\n",
    "\n",
    "chats = []\n",
    "\n",
    "for item in items:\n",
    "    item_path = os.path.join(parent_directory, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        chats.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99db163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through each chat and appending it to a dataframe, creating the baseline dataframe for Instagram\n",
    "\n",
    "num=0\n",
    "\n",
    "for chat in chats:\n",
    "    json_file_path = r'C:\\Users\\marco\\JUPYTER\\TextSentimentAnalysis\\instagram-{}\\messages\\inbox\\{}\\message_1.json'.format(insta_account, chat)\n",
    "    with open(json_file_path, encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    if num==0:\n",
    "        insta_df = pd.DataFrame.from_dict(data['messages'])\n",
    "        num=1\n",
    "    df_temp=pd.DataFrame.from_dict(data['messages'])\n",
    "    insta_df = pd.concat([insta_df, df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e59399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 69719 Instagram messages.\n"
     ]
    }
   ],
   "source": [
    "print('We have {} Instagram messages.'.format(len(insta_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bc1aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_13888\\1395765037.py:9: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  insta_df = insta_df[~insta_df['content'].str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 49900 Instagram messages.\n"
     ]
    }
   ],
   "source": [
    "# Dropping NaN values and utilizing regex to filter through dataframe with non-message content (such as likes or \n",
    "# messages containing exclusively hyperlinks)\n",
    "\n",
    "insta_df.dropna(subset=['content'], inplace=True)\n",
    "\n",
    "link_pattern = r'^(http://|https://|www\\.)\\S+$'\n",
    "insta_df = insta_df[~insta_df['content'].str.match(link_pattern, case=False)]\n",
    "\n",
    "insta_df = insta_df[~insta_df['content'].str.contains(\n",
    "    r'(?i)((.+sent an attachment.+)|(.+to your message.+)|(liked a message)|(.+shared a story.+)|\\\n",
    "    (https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})(\\.[a-zA-Z0-9]{2,})?)')]\n",
    "\n",
    "# Leaving only letters\n",
    "insta_df['content'] = insta_df['content'].apply(lambda x: ''.join(re.findall(r\"[A-Za-zÁÉÍÓÚÜáéíóúü\\s]+\", x)))\n",
    "\n",
    "print('We have {} Instagram messages.'.format(len(insta_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226758c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: te voy a, Count: 55\n",
      "Phrase: que no me, Count: 49\n",
      "Phrase: verdad es que, Count: 48\n",
      "Phrase: que no se, Count: 48\n",
      "Phrase: un par de, Count: 48\n",
      "Phrase: la verdad es, Count: 41\n",
      "Phrase: no se si, Count: 40\n",
      "Phrase: que no te, Count: 38\n",
      "Phrase: I dont know, Count: 38\n",
      "Phrase: me voy a, Count: 35\n",
      "Phrase: que se yo, Count: 34\n",
      "Phrase: de lo que, Count: 32\n",
      "Phrase: a mi me, Count: 32\n",
      "Phrase: es lo que, Count: 32\n",
      "Phrase: no me acuerdo, Count: 27\n",
      "Phrase: lo voy a, Count: 26\n",
      "Phrase: todo lo que, Count: 26\n",
      "Phrase: que no lo, Count: 26\n",
      "Phrase: un poco de, Count: 26\n",
      "Phrase: lo que te, Count: 25\n",
      "Phrase: que voy a, Count: 24\n",
      "Phrase: que es un, Count: 24\n",
      "Phrase: voy a estar, Count: 23\n",
      "Phrase: te va a, Count: 23\n",
      "Phrase: a tener que, Count: 23\n",
      "Phrase: no se que, Count: 23\n",
      "Phrase: va a ser, Count: 23\n",
      "Phrase: es algo que, Count: 23\n",
      "Phrase: I wan na, Count: 22\n",
      "Phrase: I feel like, Count: 22\n",
      "Phrase: es que no, Count: 22\n",
      "Phrase: nada que ver, Count: 22\n",
      "Phrase: no te voy, Count: 22\n",
      "Phrase: no voy a, Count: 22\n",
      "Phrase: semana que viene, Count: 21\n",
      "Phrase: a la noche, Count: 21\n",
      "Phrase: lo que me, Count: 20\n",
      "Phrase: No se si, Count: 20\n",
      "Phrase: changed the theme, Count: 20\n",
      "Phrase: the theme to, Count: 20\n",
      "Phrase: Im gon na, Count: 20\n",
      "Phrase: lo que es, Count: 20\n",
      "Phrase: lem me know, Count: 20\n",
      "Phrase: el otro dia, Count: 20\n",
      "Phrase: no pasa nada, Count: 20\n",
      "Phrase: voy a mentir, Count: 20\n",
      "Phrase: creo que es, Count: 20\n",
      "Phrase: let me know, Count: 20\n",
      "Phrase: se va a, Count: 19\n",
      "Phrase: asi que no, Count: 19\n"
     ]
    }
   ],
   "source": [
    "# Finding the most common phrases, to verify that no repetitive irrelevant messages remain\n",
    "\n",
    "phrase_length = 3\n",
    "\n",
    "# Initialize a Counter to store phrase counts\n",
    "phrase_counter = Counter()\n",
    "\n",
    "# Iterating through each row in the dataframe, tokenizing and taking a count\n",
    "for index, row in insta_df.iterrows():\n",
    "    text = str(row['content'])\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    phrases = [tuple(words[i:i+phrase_length]) for i in range(len(words) - phrase_length + 1)]\n",
    "\n",
    "    phrase_counter.update(phrases)\n",
    "\n",
    "# Now printing the most n common phrases\n",
    "top_n = 50\n",
    "most_common_phrases = phrase_counter.most_common(top_n)\n",
    "\n",
    "for phrase, count in most_common_phrases:\n",
    "    print(f'Phrase: {\" \".join(phrase)}, Count: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e09f6",
   "metadata": {},
   "source": [
    "#### Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c0d7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting folder names for each chat in order to iterate through later\n",
    "\n",
    "# insert your Facebook handle here:\n",
    "fb_account = 'marcosgrillo'\n",
    "\n",
    "parent_directory = r'C:\\Users\\marco\\JUPYTER\\TextSentimentAnalysis\\facebook-{}\\your_activity_across_facebook\\messages\\inbox'.format(fb_account)\n",
    "\n",
    "items = os.listdir(parent_directory)\n",
    "\n",
    "chats = []\n",
    "\n",
    "for item in items:\n",
    "    item_path = os.path.join(parent_directory, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        chats.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ad6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through each chat and appending it to a dataframe, creating the baseline dataframe for Facebook\n",
    "# Since Facebook stores chats in multiple jsons for longer conversations, it will be necessary to loop through them\n",
    "# individually\n",
    "\n",
    "num=0\n",
    "\n",
    "for chat in chats:\n",
    "    chat_file_path = r'C:\\Users\\marco\\JUPYTER\\TextSentimentAnalysis\\facebook-{}\\your_activity_across_facebook\\messages\\inbox\\{}'.format(fb_account, chat)\n",
    "    chat_list=[]\n",
    "    \n",
    "    # Creating a list with all the json files in a single folder\n",
    "    for filename in os.listdir(chat_file_path):\n",
    "        if filename.endswith('.json'):\n",
    "            chat_list.append(filename)\n",
    "            \n",
    "    # Compiling it all together, taking into account the different jsons\n",
    "    for log in chat_list:\n",
    "        log_file_path = r'C:\\Users\\marco\\JUPYTER\\TextSentimentAnalysis\\facebook-{}\\your_activity_across_facebook\\messages\\inbox\\{}\\{}'.format(fb_account, chat, log)\n",
    "    \n",
    "        with open(log_file_path, encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        if num==0:\n",
    "            facebook_df = pd.DataFrame.from_dict(data['messages'])\n",
    "            num=1\n",
    "        df_temp=pd.DataFrame.from_dict(data['messages'])\n",
    "        facebook_df = pd.concat([facebook_df, df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58a53b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 269463 Facebook messages.\n"
     ]
    }
   ],
   "source": [
    "print('We have {} Facebook messages.'.format(len(facebook_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e9c0815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_13888\\985993686.py:10: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  facebook_df = facebook_df[~facebook_df['content'].str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 260832 Facebook messages.\n"
     ]
    }
   ],
   "source": [
    "# Dropping NaN values and utilizing regex to filter through dataframe with non-message content (such as likes or \n",
    "# messages containing exclusively hyperlinks), as well as some particularly redundant messages I must have thought\n",
    "# very funny to send in 2011\n",
    "\n",
    "facebook_df.dropna(subset=['content'], inplace=True)\n",
    "\n",
    "link_pattern = r'^(http://|https://|www\\.)\\S+$'\n",
    "facebook_df = facebook_df[~facebook_df['content'].str.match(link_pattern, case=False)]\n",
    "\n",
    "facebook_df = facebook_df[~facebook_df['content'].str.contains(\n",
    "    r'(?i)((.+formed in Seattle.+)|(.+oO oO oO.+)|(.+Y Y Y.+)|\\\n",
    "    (https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})(\\.[a-zA-Z0-9]{2,})?)')]\n",
    "\n",
    "# Leaving only letters\n",
    "facebook_df['content'] = facebook_df['content'].apply(lambda x: ''.join(re.findall(r\"[A-Za-zÁÉÍÓÚÜáéíóúü\\s]+\", x)))\n",
    "\n",
    "print('We have {} Facebook messages.'.format(len(facebook_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48f8384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: me voy a, Count: 1312\n",
      "Phrase: left the group, Count: 516\n",
      "Phrase: que se yo, Count: 467\n",
      "Phrase: to the group, Count: 292\n",
      "Phrase: a lo de, Count: 289\n",
      "Phrase: te voy a, Count: 286\n",
      "Phrase: voy a hacer, Count: 268\n",
      "Phrase: no se si, Count: 255\n",
      "Phrase: me tengo que, Count: 252\n",
      "Phrase: un par de, Count: 248\n",
      "Phrase: lo voy a, Count: 236\n",
      "Phrase: de lo que, Count: 235\n",
      "Phrase: que no me, Count: 234\n",
      "Phrase: va a ser, Count: 231\n",
      "Phrase: es lo que, Count: 231\n",
      "Phrase: en lo de, Count: 229\n",
      "Phrase: tengo que ir, Count: 223\n",
      "Phrase: a mi me, Count: 212\n",
      "Phrase: lo que me, Count: 204\n",
      "Phrase: no voy a, Count: 198\n",
      "Phrase: no se que, Count: 198\n",
      "Phrase: que no se, Count: 197\n",
      "Phrase: ni en pedo, Count: 196\n",
      "Phrase: voy a dormir, Count: 187\n",
      "Phrase: que voy a, Count: 186\n",
      "Phrase: no creo que, Count: 185\n",
      "Phrase: lo que te, Count: 176\n",
      "Phrase: voy a comer, Count: 175\n",
      "Phrase: me dijo que, Count: 174\n",
      "Phrase: a la noche, Count: 174\n",
      "Phrase: che me voy, Count: 173\n",
      "Phrase: que ir a, Count: 172\n",
      "Phrase: a tener que, Count: 169\n",
      "Phrase: que no te, Count: 168\n",
      "Phrase: es que no, Count: 163\n",
      "Phrase: nada que ver, Count: 162\n",
      "Phrase: todo lo que, Count: 161\n",
      "Phrase: me fui a, Count: 159\n",
      "Phrase: a la mierda, Count: 155\n",
      "Phrase: tengo que hacer, Count: 153\n",
      "Phrase: te vas a, Count: 149\n",
      "Phrase: y que se, Count: 148\n",
      "Phrase: o algo asi, Count: 147\n",
      "Phrase: por lo menos, Count: 143\n",
      "Phrase: voy a ver, Count: 143\n",
      "Phrase: me parece que, Count: 142\n",
      "Phrase: se va a, Count: 142\n",
      "Phrase: el otro dia, Count: 141\n",
      "Phrase: y no me, Count: 138\n",
      "Phrase: no te preocupes, Count: 137\n"
     ]
    }
   ],
   "source": [
    "# Finding the most common phrases, to verify that no repetitive irrelevant messages remain\n",
    "\n",
    "phrase_length = 3\n",
    "\n",
    "# Initializing a Counter to store phrase counts\n",
    "phrase_counter = Counter()\n",
    "\n",
    "# Iterating through each row in the dataframe, tokenizing and taking a count\n",
    "for index, row in facebook_df.iterrows():\n",
    "    text = str(row['content'])\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    phrases = [tuple(words[i:i+phrase_length]) for i in range(len(words) - phrase_length + 1)]\n",
    "\n",
    "    phrase_counter.update(phrases)\n",
    "\n",
    "# Now printing the most n common phrases (pardon the swear words)\n",
    "top_n = 50\n",
    "most_common_phrases = phrase_counter.most_common(top_n)\n",
    "\n",
    "for phrase, count in most_common_phrases:\n",
    "    print(f'Phrase: {\" \".join(phrase)}, Count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a68fa",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664e39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Facebook and Instagram dataframes, keeping only sender, timestamp and content, since it's the only part \n",
    "# we really care about\n",
    "\n",
    "merged_df = pd.concat([facebook_df[['sender_name', 'timestamp_ms', 'content']],\n",
    "                       insta_df[['sender_name', 'timestamp_ms', 'content']]])\n",
    "\n",
    "# Eliminating excessively long messages\n",
    "\n",
    "merged_df = merged_df[merged_df['content'].str.len() < 15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2844a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting timestamp from ms to datetime\n",
    "\n",
    "merged_df['timestamp_ms'] = pd.to_datetime(merged_df['timestamp_ms'], unit='ms')\n",
    "\n",
    "# Shifting time to my local timezone from UTC\n",
    "\n",
    "merged_df['timestamp_ms'] = merged_df['timestamp_ms'] - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffcf3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling to be able to quickly pickup where I left off\n",
    "\n",
    "merged_df.to_pickle('Extracted_Transformed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d941176",
   "metadata": {},
   "source": [
    "# Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9526348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing spacy to identify the language of each text message. This takes a while to run.\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "merged_df['Language'] = None \n",
    "merged_df['Confidence'] = None\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)  # The answer to everything\n",
    "\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "# Just for getting estimates for remaining time\n",
    "start = time.time()\n",
    "\n",
    "for row in range(len(merged_df)):\n",
    "    \n",
    "    message = nlp_model(merged_df.loc[row, 'content'])\n",
    "    language = message._.language\n",
    "    \n",
    "    # Getting measures for the language and the answer confidence\n",
    "    merged_df.loc[row, 'Language'] = language['language']\n",
    "    merged_df.loc[row, 'Confidence'] = language['score']\n",
    "    \n",
    "    # Printing progress estimates periodically\n",
    "    if (row+1)%1000==0:\n",
    "        elapsed = time.time()-start\n",
    "        percent_done = (row/len(merged_df))*100\n",
    "        time_left = (elapsed/(row/len(merged_df)) - elapsed)/60\n",
    "        print('{}th row. {} % done. Time left: {} minutes'.format(row+1, percent_done, time_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45b75748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling (again!). See you in the sentiment analysis portion!\n",
    "\n",
    "merged_df.to_pickle('Extracted_Transformed_LangTagged.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
